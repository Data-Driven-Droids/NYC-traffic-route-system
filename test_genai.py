import os
import json
from typing import Optional, Dict, List, Union
from dotenv import load_dotenv
from google import genai
import asyncio


# Make sure to run: pip install google-generativeai dotenv
async def stream_gemini_response(
    prompt: str,
    history: list,
    model_name: str = "gemini-live-2.5-flash-preview",
    system_instruction: Optional[str] = None,
    tools: Optional[list] = None
):
    """
    Async generator to stream Gemini responses.

    Args:
        prompt (str): User's current prompt.
        history (list): Chat history in format [{"role": "user"/"model", "content": str}]
        model_name (str): The name of the Gemini model to use.
        system_instruction (Optional[str]): An optional system-level instruction.
        tools (Optional[list]): A list of tools, e.g., [{"google_search": {}}].

    Yields:
        str: Chunks of response text from Gemini.
    """
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        yield "Error: GOOGLE_API_KEY environment variable not set."
        return

    client = genai.Client()

    # Prepare chat history in Gemini live session format
    turns = []
    for msg in history:
        role = "user" if msg["role"] == "user" else "assistant"
        turns.append({"role": role, "parts": [{"text": msg.get("content", "")}]})

    # Add current prompt as the latest user turn
    turns.append({"role": "user", "parts": [{"text": prompt}]})

    # Build config dynamically based on provided arguments
    config = {"response_modalities": ["TEXT"]}
    if tools:
        config["tools"] = tools
    if system_instruction:
        config["system_instruction"] = system_instruction

    try:
        async with client.aio.live.connect(model=model_name, config=config) as session:
            await session.send_client_content(turns=turns)

            async for chunk in session.receive():
                if chunk.server_content:
                    if chunk.text:
                        yield chunk.text

                    # Optional: handle generated code
                    model_turn = chunk.server_content.model_turn
                    if model_turn:
                        for part in model_turn.parts:
                            if getattr(part, "executable_code", None):
                                yield f"[Code generated by model]:\n{part.executable_code.code}"
                            if getattr(part, "code_execution_result", None):
                                yield f"[Code result]:\n{part.code_execution_result.output}"

    except Exception as e:
        yield f"Error streaming Gemini response: {e}"


async def get_nyc_demographics() -> Optional[Dict[str, str]]:
    """
    Calls the stream_gemini_response function to get the latest
    population and birth rate for NYC, then parses the full response.

    This function now aggregates the streamed chunks into a single
    JSON string and then processes it.
    """
    # 1. Load environment variables
    load_dotenv()
    
    # 2. Craft the specific prompt
    prompt = """
    What is the latest estimated population and the latest reported birth rate for New York City?
    Provide the answer in a strict JSON format with two keys: "population" and "birth_rate".
    
    For example:
    {
      "population": "8.5 million (as of 2023)",
      "birth_rate": "11.2 births per 1,000 people (as of 2022)"
    }
    """
    
    # 3. Call the streaming function and aggregate the response
    full_response = ""
    print("Calling Gemini 2.5 Flash to fetch NYC demographics (via stream)...")
    
    try:
        # Use the "gemini-2.5-flash" model for this specific task
        async for chunk in stream_gemini_response(
            prompt=prompt,
            history=[],
        ):
            if chunk.startswith("Error:"):
                print(f"üö® {chunk}")
                return None
            full_response += chunk
            
        # 4. Clean and parse the aggregated JSON response
        response_text = full_response.strip().replace("```json", "").replace("```", "").strip()
        
        if not response_text:
            print("‚ö†Ô∏è ERROR: Received an empty response from the model.")
            return None
            
        data = json.loads(response_text)
        
        # 5. Perform specific validation
        if data and isinstance(data, dict) and "population" in data and "birth_rate" in data:
            print("‚úÖ Successfully fetched, aggregated, and parsed data.")
            return data
        else:
            print("‚ö†Ô∏è ERROR: The model response was missing the required 'population' or 'birth_rate' keys or was not a dict.")
            return None

    except json.JSONDecodeError:
        print(f"‚ùå ERROR: Failed to decode JSON from the aggregated model's response. Response was:\n{response_text}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return None

if __name__ == "__main__":
    # Example usage:
    # Make sure you have a .env file with your GOOGLE_API_KEY
    
    async def main():
        nyc_info = await get_nyc_demographics()
        if nyc_info:
            print("\n--- NYC Demographics ---")
            print(f"Population: {nyc_info['population']}")
            print(f"Birth Rate: {nyc_info['birth_rate']}")
        
        # Example of just streaming
        print("\n--- Streaming Test (General) ---")
        history = []
        prompt = "What are the 5 boroughs of NYC?"
        async for chunk in stream_gemini_response(
            prompt, 
            history
        ):
            print(chunk, end="", flush=True)
        print()

    asyncio.run(main())